{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change index for passenger ID and maybe get rid of iloc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following notebook I will show you the algorithmic approach I used to fill (nearly) every Cabin. This is not a guessing/probabilistic approach, cabins are filled in a structured order based on the passengers Homeplanet and its group (from its passengerID).\n",
    "\n",
    "Cabins are filled in order based on their number, ie if a passenger is in cabin A/05/P, a passenger in a later group cannot be in A/04/P but they could be in A/01/S, or B/01/P\n",
    "\n",
    "We are defining the components of the cabin by \n",
    "A/01/P\n",
    "A = cabin deck, can take values 'A','B','C','D','E','F','G','T'\n",
    "01 = cabin number, can take values 0,1,2...\n",
    "P  = cabin side, can take values 'P', 'S' (presumably 'Port' and 'Starboard' )\n",
    "\n",
    "Some assumptions\n",
    "* If two passengers are in the same group then they are on the same side, Appendix A.1\n",
    "* If two passengers are in the same group then they are from the same home planet, Appendix A.2\n",
    "* If two passengers share a last name then they are from the same home planet, Appendix A.3\n",
    "* Children <= 12 in age have no bills, Appendix A.4\n",
    "* Those who have Cryosleep = True have no bills, Appendix A.5\n",
    "* Cabins can only be shared with group members, Appendix A.6\n",
    "* Home planets restrict which decks a passenger is on, Appendix B\n",
    "** Passengers with Mars as their home planet are in decks 'D','E' or 'F'\n",
    "** Passengers with Earth as their home planet are in decks 'E','F' or 'G'\n",
    "** Passengers with Europa as their home planet are in decks 'A','B','C','D','E','T'\n",
    "** If a passenger has no bills (RoomService + ShoppingMall + Spa + VRDeck + FoodCourt) and has members in its group in different decks then they are restricted to these decks \n",
    "*** Earth :'G'\n",
    "*** Europa: 'B'\n",
    "*** Mars: 'E','F'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we combine the training data and the test data as their orders by group are preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from collections import defaultdict # Slightly modified from a regular dictionary\n",
    "\n",
    "\n",
    "training_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "training_data['Set'] = 'Train'\n",
    "test_data['Set'] = 'Test'\n",
    "\n",
    "# The combined dataframe we will be using for the rest of this project\n",
    "df = pd.concat([training_data,test_data]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our starting point, we have 299 Cabins that are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Cabin'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split their unique PassengerId into Group and their number in the group,\n",
    "we split their cabin into the deck,side and number and we split their names into first and last name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_splits(data_frame):\n",
    "    data_frame[['Group', 'GroupNumber']] = data_frame['PassengerId'].str.split('_', expand=True)\n",
    "\n",
    "    data_frame[['CabinDeck', 'CabinNumber', 'CabinSide']]= data_frame['Cabin'].str.split(\"/\", expand = True)\n",
    "    data_frame.CabinNumber = data_frame.CabinNumber.astype('Int64')\n",
    "    \n",
    "    data_frame[['FirstName','LastName']] = data_frame['Name'].str.split(\" \",expand = True)\n",
    "\n",
    "    return data_frame\n",
    "\n",
    "df = column_splits(df)\n",
    "\n",
    "\n",
    "df = df.sort_values(by = ['Group','GroupNumber'])\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total bills are composed of the summation of each passengers roomservice, foodcourt, shoppingmall, spa and vrdeck payments.\n",
    "We can impute bills to be equal to 0 if someone is under 13 and/or they are in cryosleep (Appendix A.4,A.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Bills'] = df['RoomService'] + df['FoodCourt'] + df['ShoppingMall'] + df['Spa'] + df['VRDeck']\n",
    "df.loc[(df['Age'] < 13), 'Bills'] = 0\n",
    "df.loc[(df['CryoSleep'] == True),'Bills'] = 0 \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Size is a useful column seeing how many passengers are in their group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_group_size_column(dataframe):\n",
    "    dataframe['GroupSize'] = dataframe.groupby('Group')['Group'].transform('count')\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "df = add_group_size_column(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function to impute attributes based on a shared feature, rows with missing values for their homeplanet will be imputed if they share a group with someone else with their homeplanet known or share a last name (Appendix A.2,A.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_attribute_by_shared_features(dataframe,attribute,shared_feature):\n",
    "    \n",
    "    # Iterates through all the rows that have nan for this attribute\n",
    "    for index, row in dataframe[dataframe[attribute].isna()].iterrows():\n",
    "        rows_with_shared_features = dataframe[dataframe[shared_feature] == row[shared_feature]].dropna(subset=[attribute])\n",
    "        \n",
    "        if not rows_with_shared_features.empty:\n",
    "            dataframe.loc[index, attribute] = rows_with_shared_features[attribute].iloc[0]\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "df = impute_attribute_by_shared_features(df,'HomePlanet','Group')\n",
    "df = impute_attribute_by_shared_features(df,'HomePlanet','LastName')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those who have missing Cabin values, we can limit their options by removing some cabin decks or sides that they can fit into. We know that some homeplanets can only be on some decks and that if a passenger has bills = 0 and their group are in multiple decks then they must be in a certain deck based on their home planet (Appendix B).\n",
    "\n",
    "We also know that every group is only on one cabin side even if the group is split into multiple Cabins (Appendix A.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_potential_decks_column(dataframe):\n",
    "    \n",
    "    potential_decks_by_homeplanet = {\n",
    "    'Earth':['E','F','G'],\n",
    "    'Europa': ['A','B','C','D','E','T'],\n",
    "    'Mars': ['D','E','F']\n",
    "    }\n",
    "\n",
    "    potential_decks_by_homeplanet_no_bills = {\n",
    "        'Earth':['G'],\n",
    "        'Europa':['B'],\n",
    "        'Mars': ['E','F']\n",
    "    }\n",
    "    \n",
    "    def func_potential_decks_apply(row):\n",
    "        if pd.isna(row.Cabin):\n",
    "            if row.Bills == 0 and not pd.isna(row.HomePlanet) and row.GroupSize > 1:\n",
    "                \n",
    "                group_members = dataframe[(dataframe.Group == row.Group) & (dataframe.PassengerId != row.PassengerId)].CabinDeck\n",
    "                # Checking if other members of group are in multiple different cabin decks\n",
    "                if group_members.dropna().nunique() > 1:\n",
    "                    return potential_decks_by_homeplanet_no_bills[row.HomePlanet]\n",
    "                \n",
    "                elif not group_members.isna().any():\n",
    "                    return list(set(potential_decks_by_homeplanet_no_bills[row.HomePlanet] + list(group_members.dropna().unique())))\n",
    "\n",
    "                if group_members.nunique() == 1:\n",
    "                    if group_members.iloc[0] in potential_decks_by_homeplanet_no_bills[row.HomePlanet]:\n",
    "                        return potential_decks_by_homeplanet_no_bills[row.HomePlanet]\n",
    "                    \n",
    "            # If there bills are greater than 0 then it goes to the standard decks for their homeplanet\n",
    "            if not pd.isna(row.HomePlanet):\n",
    "                return potential_decks_by_homeplanet[row.HomePlanet]\n",
    "            \n",
    "            # If their homeplanet isn't known then they could be in any cabin deck\n",
    "            else:\n",
    "                return list(dataframe.CabinDeck.dropna().unique())\n",
    "            \n",
    "    dataframe['PotentialDecks'] = dataframe.apply(func_potential_decks_apply,axis = 1)\n",
    "    return dataframe\n",
    "    \n",
    "            \n",
    "\n",
    "def add_potential_sides_column(dataframe):\n",
    "    \n",
    "    def func_potential_sides_apply(row):\n",
    "        if pd.isna(row.Cabin):\n",
    "            \n",
    "            # Checks to see if anyone else in their group has a known cabin side\n",
    "            group_sides = dataframe[dataframe.Group == row.Group].CabinSide.dropna()\n",
    "            if group_sides.nunique() > 0:\n",
    "                return [group_sides.iloc[0]]\n",
    "            \n",
    "            # If no one else is in their group or they haven't got a known cabin side then the passenger could be on either side\n",
    "            return ['P','S']\n",
    "        \n",
    "    dataframe['PotentialSides'] = dataframe.apply(func_potential_sides_apply,axis = 1)\n",
    "    return dataframe\n",
    "\n",
    "    \n",
    "df = add_potential_decks_column(df)\n",
    "df = add_potential_sides_column(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordering the values by Group then GroupNumber will be useful as this is the order we can fill up free cabins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by = ['Group','GroupNumber'])\n",
    "df = df.reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helpful function so that when we fill a cabin it'll fill the corresponding Deck, Number and Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_from_cabin_and_index(dataframe,cabin,index):\n",
    "    dataframe.loc[index,['Cabin','CabinDeck','CabinNumber','CabinSide']] = [cabin,cabin.split(\"/\")[0],int(cabin.split(\"/\")[1]),cabin.split(\"/\")[2]]\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a major function finding all the passengers missing a cabin and collecting all the cabins it could potentially fill. This is conducted by looking athe potential decks and potential sides we've found, and seeing what room number of each of those decks and sides last came beforehand and first came afterwards.\n",
    "Ie if cabin A/02/S was before and A/03/S came after there is no room in between them for the passenger to fill, \n",
    "If however Cabin A/05/P came before them and A/07/P came after, then A/06/P is a cabin it could potentially take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passengers_empty_cabin_options(dataframe):\n",
    "    \n",
    "    df_passengers_without_cabin = dataframe[dataframe['Cabin'].isna()]\n",
    "    all_passenger_cabin_options = {}\n",
    "\n",
    "    for passenger_index, passenger in df_passengers_without_cabin.iterrows():\n",
    "        all_passenger_cabin_options[passenger_index] = []\n",
    "\n",
    "        for deck in passenger.PotentialDecks:\n",
    "            for side in passenger.PotentialSides:\n",
    "                \n",
    "                # Filter dataframe for the current deck and side\n",
    "                df_filtered = dataframe[(dataframe['CabinDeck'] == deck) & (dataframe['CabinSide'] == side)]\n",
    "\n",
    "                # Split into cabins before and after the current passenger index\n",
    "                max_cabin_no_before = max(df_filtered.loc[df_filtered.index < passenger_index, 'CabinNumber'].dropna().unique(), default = -1 )\n",
    "                min_cabin_no_after = min(df_filtered.loc[df_filtered.index > passenger_index, 'CabinNumber'].dropna().unique(), default = -1)\n",
    "\n",
    "                # If no cabins were found of that deck and side before or after the row\n",
    "                if max_cabin_no_before == -1 or min_cabin_no_after == -1:\n",
    "                    continue\n",
    "                \n",
    "                # If a cabin number is seen before the row and the next cabin number is more than 1 higher after the row\n",
    "                # then there is an empty cabin it can potentially fill\n",
    "                if max_cabin_no_before + 1 < min_cabin_no_after:\n",
    "                    all_passenger_cabin_options[passenger_index] += [f\"{deck}/{i}/{side}\" for i in range(max_cabin_no_before + 1, min_cabin_no_after)]\n",
    "\n",
    "    return all_passenger_cabin_options\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# solo group and only one room that fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reasons we can't fill a cabin if the passenger has only one option of cabins from passengers_empty_cabin_options() is because the passenger also has an option to share a cabin. This function checks that they are alone in their group, meaning they can't share a cabin with anyone else as they can only share a cabin with a group member (Appendix A.6). If they are alone in their group and they have only one cabin option based on their position onboard then this cabin will be imputed for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solo_group_one_cabin_option(dataframe):\n",
    "    \n",
    "    all_passenger_cabin_options = passengers_empty_cabin_options(dataframe)\n",
    "\n",
    "    # Iterates through all the passengers that haven't got a Cabin yet and are alone in their group (ie can't share)\n",
    "    for passenger_index in list(df[(df.Cabin.isna()) & (df.GroupSize == 1)].index):\n",
    "\n",
    "        # If they have only one free cabin that they could fill\n",
    "        if len(all_passenger_cabin_options[passenger_index]) == 1:\n",
    "            matching_cabin = all_passenger_cabin_options[passenger_index][0]\n",
    "            dataframe = impute_from_cabin_and_index(dataframe,matching_cabin,passenger_index)\n",
    "\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# no free rooms so has to share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function imputes a cabin if they didn't have any empty cabin options, so will have to share a cabin with a member of their group. Some passengers have groups with multiple cabins, which we wouldn't want to guess which they would fit into, this problem can be eradicated if only one cabin that their group members are in meets their requirements we found in potential decks and potential sides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_suitable_cabin_so_shares(dataframe):\n",
    "    all_passenger_cabin_options = passengers_empty_cabin_options(dataframe)\n",
    "    \n",
    "    for passenger_index,passenger_cabin_options in all_passenger_cabin_options.items():\n",
    "        \n",
    "        # If there are no free cabins that the passenger can fill\n",
    "        if not passenger_cabin_options:\n",
    "            \n",
    "            passenger_row = dataframe.loc[passenger_index]\n",
    "            \n",
    "            # Finding all other group members cabins and filtering them by whether they are in the same deck that the passenger must be in\n",
    "            passengers_group_cabins = dataframe[(dataframe['Group'] == passenger_row['Group']) &\n",
    "                                  (dataframe['CabinDeck'].isin(passenger_row['PotentialDecks']))].Cabin.dropna()\n",
    "            \n",
    "            # If there is only one Cabin from their group they could share with\n",
    "            if passengers_group_cabins.nunique() == 1:\n",
    "                matching_cabin = passengers_group_cabins.iloc[0]\n",
    "                dataframe = impute_from_cabin_and_index(dataframe,matching_cabin,passenger_index)\n",
    "                \n",
    "    return dataframe\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# only passenger that can take that cabin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final function works based on the presumption that every cabin is filled, (ie there are no gaps in the cabin numbers), if a passenger is the only passenger that suits a certain cabin then that passenger will have that cabin allocated to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_matching_passenger_for_cabin(dataframe):\n",
    "    all_passenger_cabin_options = passengers_empty_cabin_options(dataframe)\n",
    "    \n",
    "    cabins_to_fill = defaultdict(list)\n",
    "    \n",
    "    # Iterate over cabins to see which passengers can fit that cabin\n",
    "    for passenger_index, cabin_options in all_passenger_cabin_options.items():\n",
    "        for cabin in cabin_options:\n",
    "            cabins_to_fill[cabin].append(passenger_index)\n",
    "    \n",
    "    # Iterate over cabin and impute passengers where only one fits\n",
    "    for cabin, passengers_indices in cabins_to_fill.items():\n",
    "        if len(passengers_indices) == 1:\n",
    "            dataframe = impute_from_cabin_and_index(dataframe, cabin, passengers_indices[0])\n",
    "    \n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all imputes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we impute some passengers with cabins, this limits the number of free cabins to the remaining passengers, it also reduces the competition to fill certain cabins, with my functions two iterations of them both fills all the cabins that the functions can find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def all_imputes(dataframe):\n",
    "    dataframe = solo_group_one_cabin_option(dataframe)\n",
    "    dataframe = no_suitable_cabin_so_shares(dataframe)\n",
    "    dataframe = only_matching_passenger_for_cabin(dataframe)\n",
    "\n",
    "    dataframe = solo_group_one_cabin_option(dataframe)\n",
    "    dataframe = no_suitable_cabin_so_shares(dataframe)\n",
    "    dataframe = only_matching_passenger_for_cabin(dataframe)\n",
    "    \n",
    "    return dataframe\n",
    "    \n",
    "df = all_imputes(df)\n",
    "df.Cabin.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 37 cabins that still remain unfilled, and we started with 299! There are still a few more that we can find that those functions did not cover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual workings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function prints out some useful data to help us deduce some of the remaining cabins for ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_cabin_options_for_each_row(dataframe):\n",
    "    all_passenger_cabin_options = passengers_empty_cabin_options(dataframe)\n",
    "    for passenger_index, passenger_options in all_passenger_cabin_options.items():\n",
    "        print()\n",
    "        print(\"PassengerId:\",dataframe.iloc[passenger_index].PassengerId, \"GroupSize:\", dataframe.iloc[passenger_index].GroupSize)\n",
    "        print(\"Free cabins that match:\")\n",
    "        print(passenger_options)\n",
    "\n",
    "                \n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PassengerId: 0293_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['B/13/P', 'C/13/S']\n",
      "\n",
      "PassengerId: 0310_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['B/13/P', 'C/13/S']\n",
      "\n",
      "PassengerId: 0348_02 GroupSize: 2\n",
      "Free cabins that match:\n",
      "['E/20/P', 'E/21/P']\n",
      "\n",
      "PassengerId: 0364_02 GroupSize: 2\n",
      "Free cabins that match:\n",
      "['E/20/P', 'E/21/P']\n",
      "\n",
      "PassengerId: 0374_02 GroupSize: 2\n",
      "Free cabins that match:\n",
      "['E/20/P', 'E/21/P']\n",
      "\n",
      "PassengerId: 1011_01 GroupSize: 2\n",
      "Free cabins that match:\n",
      "['E/58/P']\n",
      "\n",
      "PassengerId: 1041_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['C/40/S', 'D/36/S', 'E/58/P']\n",
      "\n",
      "PassengerId: 1095_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['C/40/S', 'D/36/S']\n",
      "\n",
      "PassengerId: 1709_03 GroupSize: 7\n",
      "Free cabins that match:\n",
      "[]\n",
      "\n",
      "PassengerId: 2092_03 GroupSize: 5\n",
      "Free cabins that match:\n",
      "[]\n",
      "\n",
      "PassengerId: 2513_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['E/150/P', 'F/519/P']\n",
      "\n",
      "PassengerId: 2514_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['E/150/P', 'F/519/P']\n",
      "\n",
      "PassengerId: 3034_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['B/98/P', 'B/99/P']\n",
      "\n",
      "PassengerId: 3053_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['B/98/P', 'B/99/P']\n",
      "\n",
      "PassengerId: 3287_02 GroupSize: 3\n",
      "Free cabins that match:\n",
      "[]\n",
      "\n",
      "PassengerId: 3411_02 GroupSize: 7\n",
      "Free cabins that match:\n",
      "[]\n",
      "\n",
      "PassengerId: 3598_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['G/590/P', 'G/579/S']\n",
      "\n",
      "PassengerId: 3599_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['G/590/P', 'G/579/S']\n",
      "\n",
      "PassengerId: 4637_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['E/300/S', 'E/301/S']\n",
      "\n",
      "PassengerId: 4652_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['E/300/S', 'E/301/S']\n",
      "\n",
      "PassengerId: 6028_04 GroupSize: 5\n",
      "Free cabins that match:\n",
      "['D/191/P']\n",
      "\n",
      "PassengerId: 6048_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['D/191/P', 'E/387/P']\n",
      "\n",
      "PassengerId: 6060_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['D/191/P', 'E/387/P']\n",
      "\n",
      "PassengerId: 7182_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['F/1489/P', 'G/1157/P']\n",
      "\n",
      "PassengerId: 7183_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['F/1489/P', 'G/1157/P']\n",
      "\n",
      "PassengerId: 7353_03 GroupSize: 3\n",
      "Free cabins that match:\n",
      "['C/270/S']\n",
      "\n",
      "PassengerId: 7368_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['C/270/S', 'D/235/P']\n",
      "\n",
      "PassengerId: 7429_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['D/235/P', 'F/1424/S']\n",
      "\n",
      "PassengerId: 7440_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['F/1424/S', 'G/1206/S']\n",
      "\n",
      "PassengerId: 7442_02 GroupSize: 2\n",
      "Free cabins that match:\n",
      "['G/1206/S']\n",
      "\n",
      "PassengerId: 7463_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['F/1544/P', 'F/1424/S', 'G/1212/S']\n",
      "\n",
      "PassengerId: 7469_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['F/1544/P', 'G/1212/S']\n",
      "\n",
      "PassengerId: 7983_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['C/298/S', 'E/528/S']\n",
      "\n",
      "PassengerId: 7995_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['C/298/S', 'E/528/S']\n",
      "\n",
      "PassengerId: 8728_07 GroupSize: 8\n",
      "Free cabins that match:\n",
      "[]\n",
      "\n",
      "PassengerId: 9223_01 GroupSize: 2\n",
      "Free cabins that match:\n",
      "['F/1785/S']\n",
      "\n",
      "PassengerId: 9223_02 GroupSize: 2\n",
      "Free cabins that match:\n",
      "['F/1785/S']\n"
     ]
    }
   ],
   "source": [
    "all_cabin_options_for_each_row(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual imputation reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Index = 1011_01, Cabin = E/58/P\n",
    "** Since the cabin can only be filled by 1011_01 and 1041_01, but one of 1041_01 and 1095_01 has to fill C/40/S and the other has to fill D/36/S as the only two that can fill those two, it leaves index 1011_01 to fill E/58/P\n",
    "* Index = 3034_01,3053_01 , Cabin = B/98/P, B/99/P\n",
    "** These indices weren't filled as the consecutive free cabins showed multiple options for each for those passengers, as no one else can fill it and index 3034_01 comes before 3053_01, they are filled in that order\n",
    "* Index = 4637_01,4652_01 , Cabin = E/300/S, E/301/S\n",
    "** As with the previous example they are the only two cabins that can fill these cabins and didn't get imputed as the free cabins are consecutive\n",
    "* Index = 6028_04, Cabin = A/57/P\n",
    "** As index 6060_01 and 6048_01 are each alone in their groups and with only two cabins to fill, one of them must fill D/191/P and one must fill E/387/P leaving index 6028_04 no other option but to join the only cabin that the rest of its group is in.\n",
    "* Index 9223_01,9223_02 Cabin = F/1785/S, F/1785/S\n",
    "** These two indices are the only members of a group together, they have one option for a cabin so they must both share F/1785/S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "cabin_list = [(1429,'E/58/P'),(4233,'B/98/P'),(4254,'B/99/P'),(6493,'E/300/S'),(6514,'E/301/S'),(8413,'A/57/P'), (12892,'F/1785/S'),(12893,'F/1785/S')]\n",
    "\n",
    "for index,cabin in cabin_list:\n",
    "    impute_from_cabin_and_index(df,cabin,index)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Cabin.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have finished with all the cabins I can find that can be imputed, we have just but 29 Cabins remaining and should help all rise up the leaderboard!\n",
    "\n",
    "Below I will detail how we can split the data back into the training set and the test set, and then further detail the reasoning behind those last 29 passengers and why we can't decie which cabin they should take (yet). If any new inferences come to light please let me know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = df[df.Set == 'Train']\n",
    "testdata = df[df.Set == 'Test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remaining Passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PassengerId: 0293_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['B/13/P', 'C/13/S']\n",
      "\n",
      "PassengerId: 0310_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['B/13/P', 'C/13/S']\n",
      "\n",
      "PassengerId: 0348_02 GroupSize: 2\n",
      "Free cabins that match:\n",
      "['E/20/P', 'E/21/P']\n",
      "\n",
      "PassengerId: 0364_02 GroupSize: 2\n",
      "Free cabins that match:\n",
      "['E/20/P', 'E/21/P']\n",
      "\n",
      "PassengerId: 0374_02 GroupSize: 2\n",
      "Free cabins that match:\n",
      "['E/20/P', 'E/21/P']\n",
      "\n",
      "PassengerId: 1041_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['C/40/S', 'D/36/S']\n",
      "\n",
      "PassengerId: 1095_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['C/40/S', 'D/36/S']\n",
      "\n",
      "PassengerId: 1709_03 GroupSize: 7\n",
      "Free cabins that match:\n",
      "[]\n",
      "\n",
      "PassengerId: 2092_03 GroupSize: 5\n",
      "Free cabins that match:\n",
      "[]\n",
      "\n",
      "PassengerId: 2513_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['E/150/P', 'F/519/P']\n",
      "\n",
      "PassengerId: 2514_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['E/150/P', 'F/519/P']\n",
      "\n",
      "PassengerId: 3287_02 GroupSize: 3\n",
      "Free cabins that match:\n",
      "[]\n",
      "\n",
      "PassengerId: 3411_02 GroupSize: 7\n",
      "Free cabins that match:\n",
      "[]\n",
      "\n",
      "PassengerId: 3598_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['G/590/P', 'G/579/S']\n",
      "\n",
      "PassengerId: 3599_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['G/590/P', 'G/579/S']\n",
      "\n",
      "PassengerId: 6048_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['D/191/P', 'E/387/P']\n",
      "\n",
      "PassengerId: 6060_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['D/191/P', 'E/387/P']\n",
      "\n",
      "PassengerId: 7182_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['F/1489/P', 'G/1157/P']\n",
      "\n",
      "PassengerId: 7183_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['F/1489/P', 'G/1157/P']\n",
      "\n",
      "PassengerId: 7353_03 GroupSize: 3\n",
      "Free cabins that match:\n",
      "['C/270/S']\n",
      "\n",
      "PassengerId: 7368_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['C/270/S', 'D/235/P']\n",
      "\n",
      "PassengerId: 7429_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['D/235/P', 'F/1424/S']\n",
      "\n",
      "PassengerId: 7440_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['F/1424/S', 'G/1206/S']\n",
      "\n",
      "PassengerId: 7442_02 GroupSize: 2\n",
      "Free cabins that match:\n",
      "['G/1206/S']\n",
      "\n",
      "PassengerId: 7463_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['F/1544/P', 'F/1424/S', 'G/1212/S']\n",
      "\n",
      "PassengerId: 7469_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['F/1544/P', 'G/1212/S']\n",
      "\n",
      "PassengerId: 7983_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['C/298/S', 'E/528/S']\n",
      "\n",
      "PassengerId: 7995_01 GroupSize: 1\n",
      "Free cabins that match:\n",
      "['C/298/S', 'E/528/S']\n",
      "\n",
      "PassengerId: 8728_07 GroupSize: 8\n",
      "Free cabins that match:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "all_cabin_options_for_each_row(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6048_01'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[8450].PassengerId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cases of two passengers alone in their group and two cabins that fit both of their requirements\n",
    "* Index 0293_01 and 0310_01, cabins B/13/P and C/13/S\n",
    "* index 1041_01 and 1095_01, cabins C/40/S and D/36/S\n",
    "* Index 2513_01 and 2514_01, Cabins E/150/P and F/519/P\n",
    "* Index 3598_01 and 3599_01, Cabins G/590/P and G/579/S\n",
    "* Index 6048_01 and 8465, Cabins D/191/P and E/387/P\n",
    "* Index 10081 and 10082, Cabins F/1489/P and G/1157/P\n",
    "* Index 10434 and 10440, Cabins F/1544/P and G/1212/S **\n",
    "* Index 11129 and 11148, Cabins C/298/S and E/528/S\n",
    "** Index 10434 has the option of  F/1424/S but as 10434 and 10440 are the only passengers that can take F/1544/P and G/1212/S one must logically take one of each\n",
    "\n",
    "\n",
    "Cases of passengers who have to share a cabin with a member of their group, but there are multiple cabins that meet their requirements\n",
    "* Index 2442, Cabins F/326/S, D/61/S, E/127/S\n",
    "* Index 2970, Cabins D/70/S, E/153/S, F/410/S\n",
    "* Index 4569, Cabins G/522/S, F/621/S\n",
    "* Index 4751, Cabins E/232/S, F/645/S\n",
    "* Index 12174, Cabins F/1798/P, G/1416/P\n",
    "\n",
    "Other cases\n",
    "* Each of index 479, 505 and 517 are in a group of 2 and are the only ones that can take cabins E/20/P and E/21/P, meaning that one of them shares with their group member and the other two take those cabins\n",
    "* Either 10290 takes cabin C/270/S and then 10313 takes D/235/P, 10394 takes F/1424/S, 10408 takes G/1206/S and 10411 shares E/495/S\n",
    "Or 10290 shares C/269/S and then 10313 takes C/270/S, 10394 takes D/235/P, 10408 takes F/1424/S and 10411 takes G/1206/S\n",
    "* 10290 10313 10394 10408 10411 10434 10440 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further assumptions,\n",
    "There aren't more cabins available than we have assumed. While I was working on this project I had missed imputing lots of cabins as if a passenger was in a later group than the highest number on a given deck and side, there was the potential that the cabin numbers could go on past what we had seen. I had given up this belief when after assuming it didn't the passengers all fit in given the constraints that seemed unlikely if this wasnt the case. Ie by way of not finding any passengers that had no options or anyone to share with, nor rooms that had no passengers that could match them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Evidence in the Appendix I will reuse the combined original dataframes without any imputations as to not misrepresent the underlying distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "df = pd.concat([training_data,test_data]) \n",
    "\n",
    "df = column_splits(df)\n",
    "df = df.sort_values(by = ['Group','GroupNumber'])\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidence of passengers sharing a group implying that their cabin is on the same side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with consistent cabin sides: 5825\n",
      "Number of rows with inconsistent cabin sides: 0\n"
     ]
    }
   ],
   "source": [
    "# Group by 'Group' and check if all non-NaN 'CabinSide' values within each group are the same\n",
    "consistent_count = 0\n",
    "inconsistent_count = 0\n",
    "\n",
    "# Iterate through each group\n",
    "for group, group_df in df.groupby('Group'):\n",
    "    if len(group_df) > 1:\n",
    "        # Get unique non-NaN CabinSide values\n",
    "        unique_sides = group_df['CabinSide'].dropna().unique()\n",
    "        \n",
    "        if len(unique_sides) <= 1:\n",
    "            # All rows in this group are consistent\n",
    "            consistent_count += len(group_df)\n",
    "        else:\n",
    "            # Some rows in this group are inconsistent\n",
    "            inconsistent_count += len(group_df)\n",
    "\n",
    "print(f\"Number of rows with consistent cabin sides: {consistent_count}\")\n",
    "print(f\"Number of rows with inconsistent cabin sides: {inconsistent_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidence of passengers sharing a group implying that they have the same home planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with consistent home planets: 5825\n",
      "Number of rows with inconsistent home planets: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters\n",
    "planet_consistent_count = 0\n",
    "planet_inconsistent_count = 0\n",
    "\n",
    "# Iterate through each group\n",
    "for group, group_df in df.groupby('Group'):\n",
    "    if len(group_df) > 1:\n",
    "    # Get unique non-NaN HomePlanet values\n",
    "        unique_home_planets = group_df['HomePlanet'].dropna().unique()\n",
    "        \n",
    "        if len(unique_home_planets) <= 1:\n",
    "            # All rows in this group are consistent in HomePlanet\n",
    "            planet_consistent_count += len(group_df)\n",
    "        else:\n",
    "            # Some rows in this group are inconsistent in HomePlanet\n",
    "            planet_inconsistent_count += len(group_df)\n",
    "\n",
    "print(f\"Number of rows with consistent home planets: {planet_consistent_count}\")\n",
    "print(f\"Number of rows with inconsistent home planets: {planet_inconsistent_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidence of passengers sharing a last name implying that they have the same home planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with consistent home planets by last name: 12468\n",
      "Number of rows with inconsistent home planets by last name: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters\n",
    "planet_consistent_count = 0\n",
    "planet_inconsistent_count = 0\n",
    "\n",
    "# Iterate through each last name group\n",
    "for last_name, group_df in df.groupby('LastName'):\n",
    "    if len(group_df) > 1:  # Exclude last names with only one passenger\n",
    "        # Get unique non-NaN HomePlanet values\n",
    "        unique_home_planets = group_df['HomePlanet'].dropna().unique()\n",
    "        \n",
    "        if len(unique_home_planets) <= 1:\n",
    "            # All rows in this group are consistent in HomePlanet\n",
    "            planet_consistent_count += len(group_df)\n",
    "        else:\n",
    "            # Some rows in this group are inconsistent in HomePlanet\n",
    "            planet_inconsistent_count += len(group_df)\n",
    "\n",
    "print(f\"Number of rows with consistent home planets by last name: {planet_consistent_count}\")\n",
    "print(f\"Number of rows with inconsistent home planets by last name: {planet_inconsistent_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidence of children under the age of 13 having no bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Bills'] = df['RoomService'] + df['FoodCourt'] + df['ShoppingMall'] + df['Spa'] + df['VRDeck']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for passengers under the age of 13 (excluding NaN Bills):\n",
      "   Total Under 13  Bills = 0  Bills != 0  Consistency Ratio\n",
      "0            1030       1030           0                1.0\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows with NaN values in Bills\n",
    "df_filtered = df[df['Bills'].notna()]\n",
    "\n",
    "# Check if passengers under the age of 13 have bills = 0\n",
    "under_13 = df_filtered[df_filtered['Age'] < 13]\n",
    "under_13_bills_zero = under_13['Bills'] == 0\n",
    "\n",
    "# Calculate summary statistics\n",
    "total_under_13 = len(under_13)\n",
    "bills_zero_under_13 = under_13_bills_zero.sum()\n",
    "bills_not_zero_under_13 = total_under_13 - bills_zero_under_13\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary = pd.DataFrame({\n",
    "    'Total Under 13': [total_under_13],\n",
    "    'Bills = 0': [bills_zero_under_13],\n",
    "    'Bills != 0': [bills_not_zero_under_13],\n",
    "    'Consistency Ratio': [bills_zero_under_13 / total_under_13]\n",
    "})\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Summary statistics for passengers under the age of 13 (excluding NaN Bills):\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidence of those in CryoSleep having no bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for passengers under the age of 13 (excluding NaN Bills):\n",
      "   Total CryoSleep  Bills == 0  Bills != 0  Consistency Ratio\n",
      "0             4068        4068           0                1.0\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows with NaN values in Bills\n",
    "df_filtered = df[df['Bills'].notna()]\n",
    "\n",
    "# Check if passengers under the age of 13 have bills = 0\n",
    "cryo = df_filtered[df_filtered['CryoSleep'] == True]\n",
    "cryo_bills_zero = cryo['Bills'] == 0\n",
    "\n",
    "# Calculate summary statistics\n",
    "total_in_cryo = len(cryo)\n",
    "cryo_bills_zero = cryo_bills_zero.sum()\n",
    "cryo_bills_not_zero = total_in_cryo - cryo_bills_zero\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary = pd.DataFrame({\n",
    "    'Total CryoSleep': [total_in_cryo],\n",
    "    'Bills == 0': [cryo_bills_zero],\n",
    "    'Bills != 0': [cryo_bills_not_zero],\n",
    "    'Consistency Ratio': [cryo_bills_zero / total_in_cryo]\n",
    "})\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Summary statistics for passengers under the age of 13 (excluding NaN Bills):\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cabins shared by more than one group (with more than one member):\n",
      "Series([], Name: Group, dtype: object)\n",
      "\n",
      "Total multi-member cabins: 1684\n",
      "Multi-member cabins shared by multiple groups: 0\n",
      "Multi-member cabins unique to one group: 1684\n"
     ]
    }
   ],
   "source": [
    "# Filter cabins with more than one member\n",
    "cabin_counts = df['Cabin'].value_counts()\n",
    "multi_member_cabins = cabin_counts[cabin_counts > 1].index\n",
    "\n",
    "# Group by Cabin and list unique groups for each Cabin with more than one member\n",
    "cabin_group_mapping = df[df['Cabin'].isin(multi_member_cabins)].groupby('Cabin')['Group'].unique()\n",
    "\n",
    "# Check if any Cabin is associated with more than one group\n",
    "shared_cabins = cabin_group_mapping[cabin_group_mapping.apply(lambda groups: len(groups) > 1)]\n",
    "\n",
    "# Print the results\n",
    "print(\"Cabins shared by more than one group (with more than one member):\")\n",
    "print(shared_cabins)\n",
    "\n",
    "# Summary statistics\n",
    "total_multi_member_cabins = len(cabin_group_mapping)\n",
    "shared_cabin_count = len(shared_cabins)\n",
    "unique_cabin_count = total_multi_member_cabins - shared_cabin_count\n",
    "\n",
    "print(f\"\\nTotal multi-member cabins: {total_multi_member_cabins}\")\n",
    "print(f\"Multi-member cabins shared by multiple groups: {shared_cabin_count}\")\n",
    "print(f\"Multi-member cabins unique to one group: {unique_cabin_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidence of home planets restricting which deck a passenger's cabin is on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HomePlanet  Earth  Europa  Mars\n",
      "CabinDeck                      \n",
      "A               0     346     0\n",
      "B               0    1124     0\n",
      "C               0    1081     0\n",
      "D               0     296   406\n",
      "E             583     197   508\n",
      "F            2426       0  1713\n",
      "G            3700       0     0\n",
      "T               0      10     0\n"
     ]
    }
   ],
   "source": [
    "# Group by 'HomePlanet' and 'CabinDeck' and count occurrences\n",
    "deck_counts = df.groupby(['HomePlanet', 'CabinDeck']).size().reset_index(name='Count')\n",
    "\n",
    "# Pivot the table to get a better overview\n",
    "pivot_table = deck_counts.pivot(index='CabinDeck', columns='HomePlanet', values='Count').fillna(0).astype(int)\n",
    "\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CabinDecks for passengers with bills = 0 in groups with multiple CabinDecks:\n",
      "HomePlanet: Earth\n",
      "  CabinDeck: G, Count: 526\n",
      "HomePlanet: Mars\n",
      "  CabinDeck: F, Count: 160\n",
      "  CabinDeck: E, Count: 47\n",
      "HomePlanet: Europa\n",
      "  CabinDeck: B, Count: 11\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Iterate through each group\n",
    "for group, group_df in df.groupby('Group'):\n",
    "    # Check if within the group there are more than one CabinDeck\n",
    "    unique_decks = group_df['CabinDeck'].dropna().unique()\n",
    "    \n",
    "    if len(unique_decks) > 1:\n",
    "        # Find passengers with bills = 0\n",
    "        zero_bill_passengers = group_df[(group_df['Bills'] == 0)  & (group_df['HomePlanet'].notna())]\n",
    "        \n",
    "        for idx, passenger in zero_bill_passengers.iterrows():\n",
    "            home_planet = passenger['HomePlanet']\n",
    "            cabin_deck = passenger['CabinDeck']\n",
    "            \n",
    "            if home_planet not in results:\n",
    "                results[home_planet] = []\n",
    "            \n",
    "            results[home_planet].append(cabin_deck)\n",
    "\n",
    "# Print the results\n",
    "print(\"CabinDecks for passengers with bills = 0 in groups with multiple CabinDecks:\")\n",
    "for home_planet, cabin_decks in results.items():\n",
    "    cabin_deck_counts = pd.Series(cabin_decks).value_counts().to_dict()\n",
    "    print(f\"HomePlanet: {home_planet}\")\n",
    "    for deck, count in cabin_deck_counts.items():\n",
    "        print(f\"  CabinDeck: {deck}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_comp = pd.read_csv('data/31remaining.csv')\n",
    "df_to_comp = df_to_comp.rename(columns = {'Number':'CabinNumber'})\n",
    "df_to_comp['CabinNumber'] = df_to_comp['CabinNumber'].astype('Int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 nan G/2/S\n",
      "66 nan B/0/S\n",
      "137 nan F/20/P\n",
      "150 nan B/5/P\n",
      "315 nan G/39/P\n",
      "331 nan E/15/S\n",
      "336 nan F/47/S\n",
      "382 nan G/45/P\n",
      "394 nan B/12/P\n",
      "412 nan G/40/S\n",
      "440 nan G/46/S\n",
      "444 nan F/70/P\n",
      "492 nan G/49/S\n",
      "529 nan B/12/S\n",
      "623 nan B/14/S\n",
      "650 nan G/73/P\n",
      "695 nan B/18/S\n",
      "701 nan E/30/S\n",
      "715 nan G/75/S\n",
      "732 nan G/79/S\n",
      "809 nan F/106/S\n",
      "982 nan C/27/S\n",
      "1003 nan A/7/S\n",
      "1016 nan B/30/P\n",
      "1042 nan F/152/P\n",
      "1049 nan G/110/S\n",
      "1069 nan A/4/P\n",
      "1077 nan C/29/S\n",
      "1175 nan F/157/S\n",
      "1178 nan G/121/P\n",
      "1188 nan C/35/S\n",
      "1244 nan E/49/P\n",
      "1251 nan F/181/P\n",
      "1319 nan F/170/S\n",
      "1401 nan D/35/S\n",
      "1423 nan E/74/S\n",
      "1429 nan E/58/P\n",
      "1441 nan G/151/P\n",
      "1574 nan F/213/S\n",
      "1601 nan G/171/P\n",
      "1603 nan E/65/P\n",
      "1605 nan B/45/S\n",
      "1631 nan C/42/P\n",
      "1712 nan F/230/S\n",
      "1759 nan F/235/S\n",
      "1842 nan E/81/P\n",
      "1863 nan E/94/S\n",
      "2013 nan B/60/S\n",
      "2059 nan G/222/P\n",
      "2082 nan C/53/P\n",
      "2113 nan G/226/S\n",
      "2146 nan F/293/P\n",
      "2222 nan F/300/S\n",
      "2229 nan C/58/S\n",
      "2250 nan F/307/P\n",
      "2267 nan F/312/P\n",
      "2311 nan F/320/P\n",
      "2357 nan G/257/S\n",
      "2369 nan F/310/S\n",
      "2408 nan G/272/P\n",
      "2414 nan G/262/S\n",
      "2421 nan G/273/P\n",
      "2454 nan E/104/P\n",
      "2457 nan G/267/S\n",
      "2537 nan G/280/S\n",
      "2592 nan F/350/S\n",
      "2677 nan C/71/S\n",
      "2689 nan D/59/P\n",
      "2813 nan C/67/P\n",
      "2987 nan G/334/S\n",
      "2997 nan G/338/S\n",
      "3001 nan B/88/S\n",
      "3035 nan G/345/P\n",
      "3052 nan F/428/P\n",
      "3101 nan F/441/P\n",
      "3124 nan G/356/S\n",
      "3194 nan G/367/P\n",
      "3277 nan D/69/P\n",
      "3362 nan G/383/S\n",
      "3416 nan G/396/P\n",
      "3481 nan G/399/S\n",
      "3484 nan E/174/S\n",
      "3507 nan F/515/P\n",
      "3560 nan G/406/S\n",
      "3569 nan A/20/P\n",
      "3590 nan E/154/P\n",
      "3652 nan G/422/P\n",
      "3701 nan E/161/P\n",
      "3770 nan F/559/P\n",
      "3989 nan F/542/S\n",
      "4057 nan F/551/S\n",
      "4089 nan F/610/P\n",
      "4142 nan F/563/S\n",
      "4233 nan B/98/P\n",
      "4254 nan B/99/P\n",
      "4303 nan D/102/S\n",
      "4365 nan G/515/P\n",
      "4389 nan E/207/S\n",
      "4438 nan G/505/S\n",
      "4577 nan B/121/S\n",
      "4661 nan G/534/S\n",
      "4689 nan F/636/S\n",
      "4886 nan D/111/S\n",
      "4914 nan G/578/P\n",
      "4915 nan B/135/S\n",
      "4931 nan F/668/S\n",
      "4957 nan D/113/S\n",
      "4985 nan F/674/S\n",
      "5084 nan G/595/P\n",
      "5193 nan G/606/S\n",
      "5211 nan F/704/S\n",
      "5225 nan F/772/P\n",
      "5260 nan E/221/P\n",
      "5264 nan G/612/P\n",
      "5343 nan F/788/P\n",
      "5346 nan F/791/P\n",
      "5540 nan G/649/S\n",
      "5565 nan B/151/S\n",
      "5570 nan F/820/P\n",
      "5614 nan G/657/P\n",
      "5626 nan F/830/P\n",
      "5655 nan G/661/S\n",
      "5748 nan D/139/P\n",
      "5782 nan E/257/P\n",
      "5787 nan F/849/P\n",
      "5823 nan G/684/S\n",
      "5857 nan C/133/P\n",
      "5989 nan B/137/P\n",
      "6019 nan F/891/P\n",
      "6075 nan C/155/S\n",
      "6117 nan E/277/P\n",
      "6127 nan C/138/P\n",
      "6133 nan C/139/P\n",
      "6148 nan F/827/S\n",
      "6159 nan G/723/P\n",
      "6227 nan G/726/S\n",
      "6472 nan B/183/S\n",
      "6492 nan G/759/S\n",
      "6493 nan E/300/S\n",
      "6514 nan E/301/S\n",
      "6535 nan C/148/P\n",
      "6545 nan G/766/S\n",
      "6578 nan B/156/P\n",
      "6709 nan C/189/S\n",
      "6728 nan E/311/S\n",
      "6745 nan G/775/P\n",
      "6750 nan G/786/S\n",
      "6781 nan F/911/S\n",
      "6821 nan F/923/S\n",
      "6927 nan C/193/S\n",
      "6952 nan A/44/P\n",
      "6956 nan F/944/S\n",
      "7011 nan G/821/S\n",
      "7035 nan F/1025/P\n",
      "7053 nan G/829/S\n",
      "7090 nan G/831/S\n",
      "7126 nan G/832/S\n",
      "7134 nan G/824/P\n",
      "7219 nan C/201/S\n",
      "7333 nan B/202/S\n",
      "7354 nan G/859/S\n",
      "7391 nan G/857/P\n",
      "7432 nan F/1093/P\n",
      "7535 nan E/344/S\n",
      "7576 nan G/874/S\n",
      "7584 nan F/1122/P\n",
      "7600 nan G/876/S\n",
      "7632 nan G/878/S\n",
      "7638 nan C/214/S\n",
      "7675 nan E/345/P\n",
      "7749 nan D/178/P\n",
      "7810 nan G/904/S\n",
      "7863 nan B/216/S\n",
      "7923 nan F/1178/P\n",
      "8015 nan F/1194/P\n",
      "8102 nan B/197/P\n",
      "8121 nan G/943/P\n",
      "8366 nan F/1237/P\n",
      "8413 nan A/57/P\n",
      "8534 nan A/58/P\n",
      "8582 nan F/1174/S\n",
      "8594 nan C/190/P\n",
      "8745 nan F/1295/P\n",
      "8804 nan G/1015/P\n",
      "8822 nan A/63/P\n",
      "8906 nan G/1026/P\n",
      "8942 nan D/200/P\n",
      "8948 nan F/1326/P\n",
      "8973 nan G/1039/S\n",
      "8976 nan B/249/S\n",
      "8988 nan G/1041/S\n",
      "8994 nan C/197/P\n",
      "9043 nan D/194/S\n",
      "9123 nan G/1053/P\n",
      "9143 nan E/414/P\n",
      "9155 nan G/1062/S\n",
      "9265 nan F/1267/S\n",
      "9267 nan F/1267/S\n",
      "9273 nan C/206/P\n",
      "9295 nan B/219/P\n",
      "9315 nan G/1085/S\n",
      "9385 nan F/1392/P\n",
      "9533 nan E/448/S\n",
      "9556 nan E/450/S\n",
      "9687 nan D/216/P\n",
      "9716 nan G/1118/P\n",
      "9787 nan D/212/S\n",
      "9825 nan G/1130/P\n",
      "9851 nan D/214/S\n",
      "9868 nan G/1134/P\n",
      "9995 nan G/1164/S\n",
      "9999 nan F/1365/S\n",
      "10050 nan F/1376/S\n",
      "10127 nan C/235/P\n",
      "10146 nan G/1178/S\n",
      "10184 nan F/1401/S\n",
      "10365 nan G/1200/S\n",
      "10460 nan C/277/S\n",
      "10558 nan C/280/S\n",
      "10786 nan E/501/P\n",
      "10804 nan C/290/S\n",
      "10855 nan B/301/S\n",
      "10877 nan G/1252/P\n",
      "10881 nan F/1492/S\n",
      "10929 nan F/1628/P\n",
      "10935 nan F/1501/S\n",
      "10965 nan G/1272/S\n",
      "10995 nan G/1273/S\n",
      "10997 nan G/1274/S\n",
      "11019 nan G/1277/P\n",
      "11037 nan E/521/S\n",
      "11056 nan G/1283/P\n",
      "11074 nan G/1286/P\n",
      "11082 nan G/1282/S\n",
      "11226 nan G/1300/S\n",
      "11247 nan E/534/S\n",
      "11334 nan G/1313/P\n",
      "11363 nan G/1317/P\n",
      "11471 nan F/1571/S\n",
      "11528 nan F/1585/S\n",
      "11583 nan F/1596/S\n",
      "11598 nan C/312/S\n",
      "11662 nan G/1364/P\n",
      "11676 nan B/328/S\n",
      "11677 nan G/1366/P\n",
      "11759 nan F/1623/S\n",
      "11767 nan E/541/P\n",
      "11835 nan F/1739/P\n",
      "11988 nan G/1394/P\n",
      "11995 nan G/1385/S\n",
      "11999 nan C/324/S\n",
      "12025 nan F/1769/P\n",
      "12084 nan G/1394/S\n",
      "12234 nan F/1808/P\n",
      "12241 nan B/339/S\n",
      "12440 nan G/1445/P\n",
      "12514 nan D/281/P\n",
      "12544 nan G/1447/S\n",
      "12653 nan D/275/S\n",
      "12658 nan G/1474/P\n",
      "12671 nan F/1757/S\n",
      "12694 nan G/1476/P\n",
      "12768 nan D/292/P\n",
      "12892 nan F/1785/S\n",
      "12893 nan F/1785/S\n",
      "12909 nan F/1889/P\n",
      "12918 nan G/1501/P\n",
      "12954 nan G/1506/P\n"
     ]
    }
   ],
   "source": [
    "for index,row in df.iterrows():\n",
    "    if not (pd.isna(row.Cabin) and pd.isna(df_to_comp.iloc[index].Cabin)):\n",
    "        if row.Cabin != df_to_comp.iloc[index].Cabin:\n",
    "            print(index,row.Cabin, df_to_comp.iloc[index].Cabin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1512\n",
      "1513\n",
      "2778\n",
      "7216\n"
     ]
    }
   ],
   "source": [
    "for index,row in df_to_comp.iterrows():\n",
    "    if row.HomePlanet == 'Earth':\n",
    "        if row.Deck == 'E':\n",
    "            if row.GroupSize == 2:\n",
    "                if row.Bills == 0:\n",
    "                    if df_to_comp[df_to_comp.Group == row.Group].Deck.nunique() == 1:\n",
    "                        print(index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spaceenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
