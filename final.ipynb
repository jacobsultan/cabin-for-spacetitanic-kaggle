{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make a discussion post and link my code post\n",
    "link it in other posts saying ive found the algorithm !\n",
    "\n",
    "change where data comes on kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "In this notebook, I will show you an algorithmic approach to fill in (nearly) every missing Cabin value in the Space Titanic dataset. Unlike probabilistic or guessing methods, this approach follows a structured order based on each passenger's Homeplanet and group (derived from their PassengerID).\n",
    "\n",
    "Cabins are filled sequentially based on their group. For instance, if a passenger is assigned to cabin A/05/P, a passenger in a subsequent group cannot be assigned to A/04/P. However, they could be assigned to A/01/S or B/01/P.\n",
    "\n",
    "By using this method, we are aiming to achieve a more accurate and logical imputation of missing Cabin values, which will improve the performance of our predictive models.\n",
    "\n",
    "## Defining Cabin and PassengerID Components and Assumptions\n",
    "\n",
    "The 'Cabin' column in the dataset is structured in the format A/01/P, where:\n",
    "\n",
    "- **A**: Cabin deck, which can take values 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'T'\n",
    "- **01**: Cabin number, which can take values 0, 1, 2, ...\n",
    "- **P**: Cabin side, which can take values 'P' (Port) or 'S' (Starboard)\n",
    "\n",
    "The 'PassengerId' column in the dataset is structured in the format 0201_01, where:\n",
    "- **0201**: Group, these codes correspond to other members of the same group\n",
    "- **01**: Group number, always starts at 1 and will count how many members there are in a group\n",
    "\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "To facilitate our imputation approach, we make several key assumptions based on all existing data following these patterns (evidence provided in the appendix):\n",
    "\n",
    "1. **Group Members Share the Same Side**: If two passengers are in the same group, they are on the same side of the ship. (Appendix A.1)\n",
    "2. **Group Members Share the Same Home Planet**: If two passengers are in the same group, they originate from the same home planet. (Appendix A.2)\n",
    "3. **Shared Last Names Indicate Same Home Planet**: Passengers sharing a last name are from the same home planet. (Appendix A.3)\n",
    "4. **Children Have No Bills**: Passengers aged 12 or younger do not have any bills. (Appendix A.4)\n",
    "5. **Cryosleep Implies No Bills**: Passengers who are in cryosleep do not have any bills. (Appendix A.5)\n",
    "6. **Cabins Shared Within Groups**: Cabins can only be shared by members of the same group. (Appendix A.6)\n",
    "7. **Home Planets and Deck Restrictions**: Passengers' home planets restrict which decks they can be assigned to. (Appendix B)\n",
    "    - **Mars**: Decks 'D', 'E', or 'F'\n",
    "    - **Earth**: Decks 'E', 'F', or 'G'\n",
    "    - **Europa**: Decks 'A', 'B', 'C', 'D', 'E', or 'T'\n",
    "    - Passengers with no bills and group members on different decks have further restrictions:\n",
    "        - **Earth**: Restricted to deck 'G'\n",
    "        - **Europa**: Restricted to deck 'B'\n",
    "        - **Mars**: Restricted to decks 'E' and 'F'\n",
    "\n",
    "\n",
    "\n",
    "To maintain the order of passengers by group, we combine the training and test datasets. This combined dataframe will be used for the rest of the project, ensuring consistency in our imputation process.\n",
    "\n",
    "By combining the datasets, we have a more comprehensive view of all passengers, which will aid in the structured filling of missing Cabin values.\n",
    "\n",
    "Below is the code to achieve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd \n",
    "\n",
    "# Slightly modified from a regular dictionary\n",
    "from collections import defaultdict \n",
    "\n",
    "# Load the training and test data\n",
    "training_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Add a column to distinguish between the training and test sets\n",
    "training_data['Set'] = 'Train'\n",
    "test_data['Set'] = 'Test'\n",
    "\n",
    "# Combine the training and test datasets\n",
    "df = pd.concat([training_data, test_data])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this section, we perform several essential preprocessing steps to prepare our data for imputation and analysis. These steps include splitting relevant columns, sorting the dataframe, and handling missing values in a structured manner.\n",
    "\n",
    "First, let's verify the number of missing 'Cabin' values in our combined dataframe:\n",
    "This is our starting point, and we have identified that there are 299 missing 'Cabin' values. Our next step will be to address some of the missing values systematically using the assumptions defined earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of missing values in the Cabin column\n",
    "df['Cabin'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We then split the unique 'PassengerId' into 'Group' and their 'GroupNumber', split the 'Cabin' into 'CabinDeck', 'CabinSide', and 'CabinNumber', and split their 'Name' into 'FirstName' and 'LastName'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>...</th>\n",
       "      <th>Name</th>\n",
       "      <th>Transported</th>\n",
       "      <th>Set</th>\n",
       "      <th>Group</th>\n",
       "      <th>GroupNumber</th>\n",
       "      <th>CabinDeck</th>\n",
       "      <th>CabinNumber</th>\n",
       "      <th>CabinSide</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>LastName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>B/0/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Maham Ofracculy</td>\n",
       "      <td>False</td>\n",
       "      <td>Train</td>\n",
       "      <td>0001</td>\n",
       "      <td>01</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>P</td>\n",
       "      <td>Maham</td>\n",
       "      <td>Ofracculy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Juanna Vines</td>\n",
       "      <td>True</td>\n",
       "      <td>Train</td>\n",
       "      <td>0002</td>\n",
       "      <td>01</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>Juanna</td>\n",
       "      <td>Vines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>58.0</td>\n",
       "      <td>True</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Altark Susent</td>\n",
       "      <td>False</td>\n",
       "      <td>Train</td>\n",
       "      <td>0003</td>\n",
       "      <td>01</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>Altark</td>\n",
       "      <td>Susent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003_02</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>33.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Solam Susent</td>\n",
       "      <td>False</td>\n",
       "      <td>Train</td>\n",
       "      <td>0003</td>\n",
       "      <td>02</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>Solam</td>\n",
       "      <td>Susent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/1/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Willy Santantines</td>\n",
       "      <td>True</td>\n",
       "      <td>Train</td>\n",
       "      <td>0004</td>\n",
       "      <td>01</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>Willy</td>\n",
       "      <td>Santantines</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  \\\n",
       "0     0001_01     Europa     False  B/0/P  TRAPPIST-1e  39.0  False   \n",
       "1     0002_01      Earth     False  F/0/S  TRAPPIST-1e  24.0  False   \n",
       "2     0003_01     Europa     False  A/0/S  TRAPPIST-1e  58.0   True   \n",
       "3     0003_02     Europa     False  A/0/S  TRAPPIST-1e  33.0  False   \n",
       "4     0004_01      Earth     False  F/1/S  TRAPPIST-1e  16.0  False   \n",
       "\n",
       "   RoomService  FoodCourt  ShoppingMall  ...               Name  Transported  \\\n",
       "0          0.0        0.0           0.0  ...    Maham Ofracculy        False   \n",
       "1        109.0        9.0          25.0  ...       Juanna Vines         True   \n",
       "2         43.0     3576.0           0.0  ...      Altark Susent        False   \n",
       "3          0.0     1283.0         371.0  ...       Solam Susent        False   \n",
       "4        303.0       70.0         151.0  ...  Willy Santantines         True   \n",
       "\n",
       "     Set Group GroupNumber CabinDeck CabinNumber CabinSide  FirstName  \\\n",
       "0  Train  0001          01         B           0         P      Maham   \n",
       "1  Train  0002          01         F           0         S     Juanna   \n",
       "2  Train  0003          01         A           0         S     Altark   \n",
       "3  Train  0003          02         A           0         S      Solam   \n",
       "4  Train  0004          01         F           1         S      Willy   \n",
       "\n",
       "      LastName  \n",
       "0    Ofracculy  \n",
       "1        Vines  \n",
       "2       Susent  \n",
       "3       Susent  \n",
       "4  Santantines  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def column_splits(data_frame):\n",
    "    # Split PassengerId into Group and GroupNumber\n",
    "    data_frame[['Group', 'GroupNumber']] = data_frame['PassengerId'].str.split('_', expand=True)\n",
    "    \n",
    "    # Split Cabin into CabinDeck, CabinNumber, and CabinSide\n",
    "    data_frame[['CabinDeck', 'CabinNumber', 'CabinSide']] = data_frame['Cabin'].str.split(\"/\", expand=True)\n",
    "    data_frame['CabinNumber'] = data_frame['CabinNumber'].astype('Int64')\n",
    "    \n",
    "    # Split Name into FirstName and LastName\n",
    "    data_frame[['FirstName', 'LastName']] = data_frame['Name'].str.split(\" \", expand=True)\n",
    "\n",
    "    return data_frame\n",
    "\n",
    "# Apply the function to the combined dataframe\n",
    "df = column_splits(df)\n",
    "\n",
    "# Sort the dataframe by Group and GroupNumber\n",
    "df = df.sort_values(by=['Group', 'GroupNumber'])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows of the modified dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The total 'Bills' are composed of the summation of each passengers 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa' and 'VRDeck' payments.\n",
    "We can impute bills to be equal to 0 if someone is under 13 and/or they are in cryosleep (Appendix A.4,A.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate total bills for each passenger\n",
    "df['Bills'] = df['RoomService'] + df['FoodCourt'] + df['ShoppingMall'] + df['Spa'] + df['VRDeck']\n",
    "\n",
    "# Impute bills to be zero for passengers under 13 or in cryosleep\n",
    "df.loc[df['Age'] < 13, 'Bills'] = 0\n",
    "df.loc[df['CryoSleep'] == True, 'Bills'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We then add a useful column to our dataframe: 'GroupSize', which indicates the number of passengers in each group. This will assist in the imputation process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_group_size_column(dataframe):\n",
    "    dataframe['GroupSize'] = dataframe.groupby('Group')['Group'].transform('count')\n",
    "    return dataframe\n",
    "\n",
    "# Apply the function to the combined dataframe\n",
    "df = add_group_size_column(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to impute missing values based on shared features. For instance, rows with missing values for HomePlanet can be imputed if they share a group with someone whose HomePlanet is known or share a last name with someone whose HomePlanet is known (Appendix A.2, A.3):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_attribute_by_shared_features(dataframe, attribute, shared_feature):\n",
    "    # Iterate through rows with missing values for the specified attribute\n",
    "    for index, row in dataframe[dataframe[attribute].isna()].iterrows():\n",
    "        \n",
    "        # Find rows that share the specified feature and have known values for the attribute\n",
    "        rows_with_shared_features = dataframe[dataframe[shared_feature] == row[shared_feature]].dropna(subset=[attribute])\n",
    "        \n",
    "        # Impute the attribute if there are rows with shared features and known values\n",
    "        if not rows_with_shared_features.empty:\n",
    "            dataframe.loc[index, attribute] = rows_with_shared_features[attribute].iloc[0]\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "# Impute missing HomePlanet values based on shared group or last name\n",
    "df = impute_attribute_by_shared_features(df, 'HomePlanet', 'Group')\n",
    "df = impute_attribute_by_shared_features(df, 'HomePlanet', 'LastName')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Determining Potential Cabin Decks and Sides\n",
    "\n",
    "For passengers with missing 'Cabin' values, we can limit their options by excluding certain cabin decks or sides based on their attributes. This step is crucial for our imputation process. Specifically:\n",
    "\n",
    "1. **Home Planet Deck Restrictions**: Each home planet restricts passengers to certain decks. Additionally, if a passenger has no bills and their group members are on multiple decks, their deck is further restricted based on their home planet (Appendix B).\n",
    "2. **Group Cabin Side Consistency**: Every group is restricted to a single cabin side, even if they are split into multiple cabins (Appendix A.1).\n",
    "\n",
    "We define functions to add columns for potential decks and sides for passengers with missing Cabin values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_potential_decks_column(dataframe):\n",
    "    \n",
    "    # Restricted decks for passengers\n",
    "    potential_decks_by_homeplanet = {\n",
    "        'Earth': ['E', 'F', 'G'],\n",
    "        'Europa': ['A', 'B', 'C', 'D', 'E', 'T'],\n",
    "        'Mars': ['D', 'E', 'F']\n",
    "    }\n",
    "\n",
    "    # Restricted decks for passengers with no bills\n",
    "    potential_decks_by_homeplanet_no_bills = {\n",
    "        'Earth': ['G'],\n",
    "        'Europa': ['B'],\n",
    "        'Mars': ['E', 'F']\n",
    "    }\n",
    "    \n",
    "    # Inner function to determine potential decks for each passenger\n",
    "    def func_potential_decks_apply(row):\n",
    "        \n",
    "        # If the Cabin value is missing\n",
    "        if pd.isna(row.Cabin):\n",
    "            \n",
    "            # If the passenger has no bills, a known HomePlanet, and is part of a group\n",
    "            if row.Bills == 0 and not pd.isna(row.HomePlanet) and row.GroupSize > 1:\n",
    "                \n",
    "                # Get the decks of other group members\n",
    "                group_members = dataframe[(dataframe.Group == row.Group) & (dataframe.PassengerId != row.PassengerId)].CabinDeck\n",
    "            \n",
    "                # If there are multiple unique decks in the group, return the restricted decks for the HomePlanet\n",
    "                if group_members.dropna().nunique() > 1:\n",
    "                    return potential_decks_by_homeplanet_no_bills[row.HomePlanet]\n",
    "                \n",
    "                # Otherwise, They could be in the same deck as the group members or in the restricted decks\n",
    "                else:\n",
    "                    return list(set(potential_decks_by_homeplanet_no_bills[row.HomePlanet] + list(group_members.dropna().unique())))\n",
    "        \n",
    "        \n",
    "            # If the passenger has bills, or is alone in its group return the standard decks for their HomePlanet\n",
    "            elif not pd.isna(row.HomePlanet):\n",
    "                return potential_decks_by_homeplanet[row.HomePlanet]\n",
    "            \n",
    "            # If the HomePlanet is unknown, return all decks in the dataframe\n",
    "            else:\n",
    "                return list(dataframe.CabinDeck.dropna().unique())\n",
    "    \n",
    "    # Apply the inner function to each row in the dataframe\n",
    "    dataframe['PotentialDecks'] = dataframe.apply(func_potential_decks_apply, axis=1)\n",
    "    return dataframe\n",
    "\n",
    "def add_potential_sides_column(dataframe):\n",
    "    \n",
    "    # Inner function to determine potential sides for each passenger\n",
    "    def func_potential_sides_apply(row):\n",
    "        \n",
    "        # If the Cabin value is missing\n",
    "        if pd.isna(row.Cabin):\n",
    "            \n",
    "            # Get the sides of other group members\n",
    "            group_sides = dataframe[dataframe.Group == row.Group].CabinSide.dropna()\n",
    "            \n",
    "            # If other group members have a known side, return that side\n",
    "            if group_sides.nunique() > 0:\n",
    "                return [group_sides.iloc[0]]\n",
    "            \n",
    "            # If no group members have a known side, return both possible sides\n",
    "            return ['P', 'S']\n",
    "        \n",
    "    # Apply the inner function to each row in the dataframe\n",
    "    dataframe['PotentialSides'] = dataframe.apply(func_potential_sides_apply, axis=1)\n",
    "    return dataframe\n",
    "\n",
    "# Apply the functions to add potential decks and sides columns\n",
    "df = add_potential_decks_column(df)\n",
    "df = add_potential_sides_column(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "By adding these columns, we can better manage the imputation of missing Cabin values by limiting the possible options based on the passenger's attributes and group sizes.\n",
    "\n",
    "Sorting the dataframe by Group and GroupNumber is useful as it allows us to fill in free cabins in a logical order.\n",
    "\n",
    "With this preparation, we are now ready to proceed with the structured imputation of missing Cabin values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>...</th>\n",
       "      <th>GroupNumber</th>\n",
       "      <th>CabinDeck</th>\n",
       "      <th>CabinNumber</th>\n",
       "      <th>CabinSide</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>LastName</th>\n",
       "      <th>Bills</th>\n",
       "      <th>GroupSize</th>\n",
       "      <th>PotentialDecks</th>\n",
       "      <th>PotentialSides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>B/0/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>01</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>P</td>\n",
       "      <td>Maham</td>\n",
       "      <td>Ofracculy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>01</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>Juanna</td>\n",
       "      <td>Vines</td>\n",
       "      <td>736.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>58.0</td>\n",
       "      <td>True</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>01</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>Altark</td>\n",
       "      <td>Susent</td>\n",
       "      <td>10383.0</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003_02</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>33.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>...</td>\n",
       "      <td>02</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>Solam</td>\n",
       "      <td>Susent</td>\n",
       "      <td>5176.0</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/1/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>...</td>\n",
       "      <td>01</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>Willy</td>\n",
       "      <td>Santantines</td>\n",
       "      <td>1091.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  \\\n",
       "0     0001_01     Europa     False  B/0/P  TRAPPIST-1e  39.0  False   \n",
       "1     0002_01      Earth     False  F/0/S  TRAPPIST-1e  24.0  False   \n",
       "2     0003_01     Europa     False  A/0/S  TRAPPIST-1e  58.0   True   \n",
       "3     0003_02     Europa     False  A/0/S  TRAPPIST-1e  33.0  False   \n",
       "4     0004_01      Earth     False  F/1/S  TRAPPIST-1e  16.0  False   \n",
       "\n",
       "   RoomService  FoodCourt  ShoppingMall  ...  GroupNumber  CabinDeck  \\\n",
       "0          0.0        0.0           0.0  ...           01          B   \n",
       "1        109.0        9.0          25.0  ...           01          F   \n",
       "2         43.0     3576.0           0.0  ...           01          A   \n",
       "3          0.0     1283.0         371.0  ...           02          A   \n",
       "4        303.0       70.0         151.0  ...           01          F   \n",
       "\n",
       "  CabinNumber CabinSide FirstName     LastName    Bills GroupSize  \\\n",
       "0           0         P     Maham    Ofracculy      0.0         1   \n",
       "1           0         S    Juanna        Vines    736.0         1   \n",
       "2           0         S    Altark       Susent  10383.0         2   \n",
       "3           0         S     Solam       Susent   5176.0         2   \n",
       "4           1         S     Willy  Santantines   1091.0         1   \n",
       "\n",
       "   PotentialDecks PotentialSides  \n",
       "0            None           None  \n",
       "1            None           None  \n",
       "2            None           None  \n",
       "3            None           None  \n",
       "4            None           None  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Imputing Algorithm for Missing Cabin Values !\n",
    "\n",
    "To accurately impute missing 'Cabin' values, we create several helper functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Helper Function to Impute Cabin Details\n",
    "\n",
    "First, we define a function to ensure that when a cabin is filled, the corresponding 'CabinDeck', 'CabinNumber', and 'CabinSide' are also updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def impute_from_cabin_and_index(dataframe, cabin, index):\n",
    "    \n",
    "    # Split the cabin string into Deck, Number, and Side\n",
    "    cabin_deck = cabin.split(\"/\")[0]\n",
    "    cabin_number = int(cabin.split(\"/\")[1])\n",
    "    cabin_side = cabin.split(\"/\")[2]\n",
    "    \n",
    "    # Update the dataframe with the cabin details\n",
    "    dataframe.loc[index, ['Cabin', 'CabinDeck', 'CabinNumber', 'CabinSide']] = [cabin, cabin_deck, cabin_number, cabin_side]\n",
    "    \n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Finding Potential Cabin Options\n",
    "\n",
    "\n",
    "Next, we define a major function to find all the passengers without a cabin and collect all the cabins they could fill. This is done by examining their potential decks and sides, and their position dictated by their group. For instance, if Cabin A/02/S was filled by a passenger in a group before them and A/03/S by a passenger in a group after them came, there is no room on deck 'A' and side 'S' for the passenger to fill. However, if Cabin A/05/P came before and A/07/P was filled after and no one was in A/06/P, it would be a potential cabin the passenger could take.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passengers_cabin_options(dataframe):\n",
    "    # Filter dataframe to find passengers without a cabin\n",
    "    df_passengers_without_cabin = dataframe[dataframe['Cabin'].isna()]\n",
    "    \n",
    "    # Dictionary to store cabin options for each passenger\n",
    "    all_passenger_cabin_options = {}\n",
    "\n",
    "    # Iterate through each passenger without a cabin\n",
    "    for passenger_index, passenger in df_passengers_without_cabin.iterrows():\n",
    "        all_passenger_cabin_options[passenger_index] = []\n",
    "\n",
    "        for deck in passenger.PotentialDecks:\n",
    "            for side in passenger.PotentialSides:\n",
    "                \n",
    "                # Filter dataframe for the current deck and side\n",
    "                df_filtered = dataframe[(dataframe['CabinDeck'] == deck) & (dataframe['CabinSide'] == side)]\n",
    "\n",
    "                # Find the maximum cabin number before the current passenger index\n",
    "                cabins_before = df_filtered.loc[df_filtered.index < passenger_index, 'CabinNumber'].dropna().unique()\n",
    "                if cabins_before.size > 0:\n",
    "                    max_cabin_no_before = max(cabins_before)\n",
    "                    \n",
    "                # If there are no cabins before the current passenger index\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Find the minimum cabin number after the current passenger index\n",
    "                cabins_after = df_filtered.loc[df_filtered.index > passenger_index, 'CabinNumber'].dropna().unique()\n",
    "                if cabins_after.size > 0:\n",
    "                    min_cabin_no_after = min(cabins_after)\n",
    "                    \n",
    "                # If there are no cabins after the current passenger index\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # If there is a gap between the maximum cabin number before and the minimum cabin number after then there are potential cabins the passenger can fill\n",
    "                if max_cabin_no_before + 1 < min_cabin_no_after:\n",
    "                    potential_cabins = [f\"{deck}/{i}/{side}\" for i in range(max_cabin_no_before + 1, min_cabin_no_after)]\n",
    "                    all_passenger_cabin_options[passenger_index].extend(potential_cabins)\n",
    "        \n",
    "        # If the passenger can share with someone in its group then put that as a cabin option\n",
    "        if passenger.GroupSize > 1:\n",
    "            for cabin in dataframe[dataframe['Group'] == passenger.Group]['Cabin'].dropna().unique():\n",
    "                \n",
    "                # Check that cabin is compatible with passenger\n",
    "                cabin_deck, cabin_side = cabin.split(\"/\")[0], cabin.split(\"/\")[2]\n",
    "                if cabin_deck in passenger.PotentialDecks and cabin_side in passenger.PotentialSides:\n",
    "                    all_passenger_cabin_options[passenger_index].append(cabin)\n",
    "\n",
    "    return all_passenger_cabin_options\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By identifying the potential cabins for each passenger, we can systematically fill in the missing Cabin values. This ensures that each imputation is consistent with the constraints and assumptions defined earlier.\n",
    "\n",
    "In the next section, we will use these passenger cabin options to impute the missing values and complete our dataset.\n",
    "\n",
    "## Imputing Cabins for Passengers with Only One Cabin Option\n",
    "\n",
    "If a passenger is alone in a group and there is only one free cabin it can fill, we know that it must be in that cabin, alternatively if it can't fill any empty cabins, and there is only one cabin in its group that it can share, it must take that one.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If only one cabin is available for a passenger then impute it\n",
    "def impute_single_cabin_option(dataframe):\n",
    "    # Get all passengers looking for a cabin ands its cabin options\n",
    "    all_passenger_cabin_options = passengers_cabin_options(dataframe)\n",
    "    for passenger_index, cabin_options in all_passenger_cabin_options.items():\n",
    "        # If there is only one cabin option\n",
    "        if len(cabin_options) == 1:\n",
    "            dataframe = impute_from_cabin_and_index(dataframe, cabin_options[0], passenger_index)\n",
    "            \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Imputing Cabins for the Only Passenger That Can Take a Certain Cabin\n",
    "\n",
    "This final function works based on the assumption that every cabin is filled (i.e., there are no gaps in the cabin numbers). If a passenger is the only one that suits a certain cabin, then that passenger will have that cabin allocated to them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_matching_passenger_for_cabin(dataframe):\n",
    "    # Get potential cabin options for passengers missing a cabin\n",
    "    all_passenger_cabin_options = passengers_cabin_options(dataframe)\n",
    "    \n",
    "    # Dictionary to store which passengers can fit each cabin\n",
    "    cabins_to_fill = defaultdict(list)\n",
    "    \n",
    "    # Iterate over each passenger and their potential cabin options\n",
    "    for passenger_index, cabin_options in all_passenger_cabin_options.items():\n",
    "        for cabin in cabin_options:\n",
    "            # if no passengers are in the cabin then add it to the list\n",
    "            if dataframe[dataframe['Cabin'] == cabin].shape[0] == 0:\n",
    "                cabins_to_fill[cabin].append(passenger_index)\n",
    "    \n",
    "    # Iterate over each cabin and impute passengers where only one passenger fits\n",
    "    for cabin, passengers_indices in cabins_to_fill.items():\n",
    "        if len(passengers_indices) == 1:\n",
    "            dataframe = impute_from_cabin_and_index(dataframe, cabin, passengers_indices[0])\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Imputation Process\n",
    "\n",
    "As we impute cabins for some passengers, it limits the number of free cabins available for the remaining passengers and reduces the competition to fill certain cabins. By iterating through our imputation functions twice, we ensure that all the cabins that can be found by the functions are filled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def all_imputes(dataframe):\n",
    "    dataframe = impute_single_cabin_option(dataframe)\n",
    "    dataframe = only_matching_passenger_for_cabin(dataframe)\n",
    "    \n",
    "    dataframe = impute_single_cabin_option(dataframe)\n",
    "    dataframe = only_matching_passenger_for_cabin(dataframe)\n",
    "    return dataframe\n",
    "\n",
    "# Apply the all_imputes function to the dataframe\n",
    "df = all_imputes(df)\n",
    "\n",
    "# Check the number of missing values in the Cabin column after imputation\n",
    "df.Cabin.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this final imputation process, we have successfully filled as many missing Cabin values as possible based on the constraints and assumptions defined earlier.\n",
    "\n",
    "There are 35 cabins that still remain unfilled, and we started with 299! There are still a few more that we can find that those algorithms did not cover\n",
    "\n",
    "## Manual Imputation of Remaining Cabins\n",
    "\n",
    "We can manually deduce some of the remaining cabins using a helper function that prints out useful data for each passenger with missing cabin information. This function will provide insights into the potential cabin options for each passenger, which can help us manually impute the remaining cabins.\n",
    "\n",
    "### Function to Print Cabin Options for Each Passenger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def all_cabin_options_for_each_row(dataframe):\n",
    "    all_passenger_cabin_options = passengers_cabin_options(dataframe)\n",
    "    \n",
    "\n",
    "    for passenger_index, passenger_options in all_passenger_cabin_options.items():\n",
    "        print()\n",
    "        print(\"PassengerId:\", dataframe.iloc[passenger_index].PassengerId, \"GroupSize:\", dataframe.iloc[passenger_index].GroupSize)\n",
    "        print(\"Cabins that match:\")\n",
    "        print(passenger_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PassengerId: 0293_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['B/13/P', 'C/13/S']\n",
      "\n",
      "PassengerId: 0310_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['B/13/P', 'C/13/S']\n",
      "\n",
      "PassengerId: 0348_02 GroupSize: 2\n",
      "Cabins that match:\n",
      "['E/20/P', 'E/21/P', 'D/12/P']\n",
      "\n",
      "PassengerId: 0364_02 GroupSize: 2\n",
      "Cabins that match:\n",
      "['E/20/P', 'E/21/P', 'F/81/P']\n",
      "\n",
      "PassengerId: 0374_02 GroupSize: 2\n",
      "Cabins that match:\n",
      "['E/20/P', 'E/21/P', 'F/86/P']\n",
      "\n",
      "PassengerId: 1011_01 GroupSize: 2\n",
      "Cabins that match:\n",
      "['E/58/P', 'G/148/P']\n",
      "\n",
      "PassengerId: 1041_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['C/40/S', 'D/36/S', 'E/58/P']\n",
      "\n",
      "PassengerId: 1095_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['C/40/S', 'D/36/S']\n",
      "\n",
      "PassengerId: 1709_03 GroupSize: 7\n",
      "Cabins that match:\n",
      "['F/326/S', 'D/61/S', 'E/127/S']\n",
      "\n",
      "PassengerId: 2092_03 GroupSize: 5\n",
      "Cabins that match:\n",
      "['D/70/S', 'E/153/S', 'F/410/S']\n",
      "\n",
      "PassengerId: 2513_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['E/150/P', 'F/519/P']\n",
      "\n",
      "PassengerId: 2514_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['E/150/P', 'F/519/P']\n",
      "\n",
      "PassengerId: 3034_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['B/98/P', 'B/99/P']\n",
      "\n",
      "PassengerId: 3053_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['B/98/P', 'B/99/P']\n",
      "\n",
      "PassengerId: 3287_02 GroupSize: 3\n",
      "Cabins that match:\n",
      "['G/522/S', 'F/621/S']\n",
      "\n",
      "PassengerId: 3411_02 GroupSize: 7\n",
      "Cabins that match:\n",
      "['E/232/S', 'F/645/S']\n",
      "\n",
      "PassengerId: 3598_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['G/590/P', 'G/579/S']\n",
      "\n",
      "PassengerId: 3599_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['G/590/P', 'G/579/S']\n",
      "\n",
      "PassengerId: 4637_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['E/300/S', 'E/301/S']\n",
      "\n",
      "PassengerId: 4652_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['E/300/S', 'E/301/S']\n",
      "\n",
      "PassengerId: 6028_04 GroupSize: 5\n",
      "Cabins that match:\n",
      "['D/191/P', 'A/57/P']\n",
      "\n",
      "PassengerId: 6048_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['D/191/P', 'E/387/P']\n",
      "\n",
      "PassengerId: 6060_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['D/191/P', 'E/387/P']\n",
      "\n",
      "PassengerId: 7182_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['F/1489/P', 'G/1157/P']\n",
      "\n",
      "PassengerId: 7183_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['F/1489/P', 'G/1157/P']\n",
      "\n",
      "PassengerId: 7353_03 GroupSize: 3\n",
      "Cabins that match:\n",
      "['C/270/S', 'C/269/S']\n",
      "\n",
      "PassengerId: 7368_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['C/270/S', 'D/235/P']\n",
      "\n",
      "PassengerId: 7429_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['D/235/P', 'F/1424/S']\n",
      "\n",
      "PassengerId: 7440_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['F/1424/S', 'G/1206/S']\n",
      "\n",
      "PassengerId: 7442_02 GroupSize: 2\n",
      "Cabins that match:\n",
      "['G/1206/S', 'E/495/S']\n",
      "\n",
      "PassengerId: 7463_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['F/1544/P', 'F/1424/S', 'G/1212/S']\n",
      "\n",
      "PassengerId: 7469_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['F/1544/P', 'G/1212/S']\n",
      "\n",
      "PassengerId: 7983_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['C/298/S', 'E/528/S']\n",
      "\n",
      "PassengerId: 7995_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['C/298/S', 'E/528/S']\n",
      "\n",
      "PassengerId: 8728_07 GroupSize: 8\n",
      "Cabins that match:\n",
      "['F/1798/P', 'G/1416/P']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_cabin_options_for_each_row(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Imputation Reasoning\n",
    "\n",
    "By analyzing the potential options for these passengers manually, we can deduce some further cabins to be imputed.\n",
    "\n",
    "### Manual Imputation Details\n",
    "\n",
    "1. **Passenger 1011_01, Cabin E/58/P**\n",
    "   - The cabin can only be filled by Passengers 1011_01 and 1041_01. However, one of 1041_01 and 1095_01 has to fill C/40/S, and the other has to fill D/36/S, as they are the only two that can fill these cabins. This leaves passenger 1011_01 to be the only option to fill E/58/P.\n",
    "\n",
    "2. **Passengers 3034_01, 3053_01, Cabins B/98/P, B/99/P**\n",
    "   - These passengers weren't filled as the consecutive free cabins showed multiple options. As no one else can fill these cabins and index 3034_01 comes before 3053_01, they are filled in that order.\n",
    "\n",
    "3. **Passengers 4637_01, 4652_01, Cabins E/300/S, E/301/S**\n",
    "   - These passengers are the only two that can fill these cabins. They weren't imputed earlier as the free cabins are consecutive.\n",
    "\n",
    "4. **Passenger 6028_04, Cabin A/57/P**\n",
    "   - Since 6060_01 and 6048_01 are each alone in their groups and both can only be in cabins D/191/P or E/387/P, one must fill D/191/P, and the other must fill E/387/P. This leaves index 6028_04 no other option but to join the only cabin that the rest of its group is in.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# List of manually determined cabin assignments\n",
    "manual_cabins_to_impute = [\n",
    "    (1429, 'E/58/P'), (4233, 'B/98/P'), (4254, 'B/99/P'), \n",
    "    (6493, 'E/300/S'), (6514, 'E/301/S'), (8413, 'A/57/P'), \n",
    "]\n",
    "\n",
    "# Apply the manual imputation\n",
    "for index, cabin in manual_cabins_to_impute:\n",
    "    df = impute_from_cabin_and_index(df, cabin, index)\n",
    "\n",
    "# Check the number of missing values in the Cabin column after manual imputation\n",
    "df.Cabin.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "**Thank you for making it this far! This project required a significant amount of effort and dedication to address the complex problem of imputing missing cabin values. By analyzing the data and applying structured algorithms, we have managed to reduce the number of missing cabins from 299 to just 29!**\n",
    "\n",
    "**I hope that the techniques and insights shared in this notebook will be beneficial for your own projects and help you climb the leaderboard!! If you found this work helpful, I would greatly appreciate your upvotes and feedback. It would mean a lot to know that my contributions are making a positive impact.**\n",
    "\n",
    "**Best of luck with your submissions, if you have any further questions or if new inferences come to light, please feel free to reach out :). Together, we can continue to improve and refine our approaches.**\n",
    "\n",
    "**Thank you once again for your time and effort in reviewing this work. Your support and encouragement are much appreciated!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Next, we will split the data back into the training and test sets. Additionally, I will detail the reasoning behind the remaining 29 passengers and explain why we cannot yet decide which cabin they should take. If anything comes to mind please let me know\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data back into training and test sets\n",
    "traindata = df[df.Set == 'Train']\n",
    "testdata = df[df.Set == 'Test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Remaining Missing Cabins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PassengerId: 0293_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['B/13/P', 'C/13/S']\n",
      "\n",
      "PassengerId: 0310_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['B/13/P', 'C/13/S']\n",
      "\n",
      "PassengerId: 0348_02 GroupSize: 2\n",
      "Cabins that match:\n",
      "['E/20/P', 'E/21/P', 'D/12/P']\n",
      "\n",
      "PassengerId: 0364_02 GroupSize: 2\n",
      "Cabins that match:\n",
      "['E/20/P', 'E/21/P', 'F/81/P']\n",
      "\n",
      "PassengerId: 0374_02 GroupSize: 2\n",
      "Cabins that match:\n",
      "['E/20/P', 'E/21/P', 'F/86/P']\n",
      "\n",
      "PassengerId: 1041_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['C/40/S', 'D/36/S']\n",
      "\n",
      "PassengerId: 1095_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['C/40/S', 'D/36/S']\n",
      "\n",
      "PassengerId: 1709_03 GroupSize: 7\n",
      "Cabins that match:\n",
      "['F/326/S', 'D/61/S', 'E/127/S']\n",
      "\n",
      "PassengerId: 2092_03 GroupSize: 5\n",
      "Cabins that match:\n",
      "['D/70/S', 'E/153/S', 'F/410/S']\n",
      "\n",
      "PassengerId: 2513_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['E/150/P', 'F/519/P']\n",
      "\n",
      "PassengerId: 2514_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['E/150/P', 'F/519/P']\n",
      "\n",
      "PassengerId: 3287_02 GroupSize: 3\n",
      "Cabins that match:\n",
      "['G/522/S', 'F/621/S']\n",
      "\n",
      "PassengerId: 3411_02 GroupSize: 7\n",
      "Cabins that match:\n",
      "['E/232/S', 'F/645/S']\n",
      "\n",
      "PassengerId: 3598_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['G/590/P', 'G/579/S']\n",
      "\n",
      "PassengerId: 3599_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['G/590/P', 'G/579/S']\n",
      "\n",
      "PassengerId: 6048_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['D/191/P', 'E/387/P']\n",
      "\n",
      "PassengerId: 6060_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['D/191/P', 'E/387/P']\n",
      "\n",
      "PassengerId: 7182_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['F/1489/P', 'G/1157/P']\n",
      "\n",
      "PassengerId: 7183_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['F/1489/P', 'G/1157/P']\n",
      "\n",
      "PassengerId: 7353_03 GroupSize: 3\n",
      "Cabins that match:\n",
      "['C/270/S', 'C/269/S']\n",
      "\n",
      "PassengerId: 7368_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['C/270/S', 'D/235/P']\n",
      "\n",
      "PassengerId: 7429_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['D/235/P', 'F/1424/S']\n",
      "\n",
      "PassengerId: 7440_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['F/1424/S', 'G/1206/S']\n",
      "\n",
      "PassengerId: 7442_02 GroupSize: 2\n",
      "Cabins that match:\n",
      "['G/1206/S', 'E/495/S']\n",
      "\n",
      "PassengerId: 7463_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['F/1544/P', 'F/1424/S', 'G/1212/S']\n",
      "\n",
      "PassengerId: 7469_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['F/1544/P', 'G/1212/S']\n",
      "\n",
      "PassengerId: 7983_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['C/298/S', 'E/528/S']\n",
      "\n",
      "PassengerId: 7995_01 GroupSize: 1\n",
      "Cabins that match:\n",
      "['C/298/S', 'E/528/S']\n",
      "\n",
      "PassengerId: 8728_07 GroupSize: 8\n",
      "Cabins that match:\n",
      "['F/1798/P', 'G/1416/P']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_cabin_options_for_each_row(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Cases of two passengers alone in their groups with only two Cabins they can fill\n",
    "1. **Passengers 0293_01 and 0310_01**: Cabins B/13/P and C/13/S\n",
    "2. **Passengers 1041_01 and 1095_01**: Cabins C/40/S and D/36/S\n",
    "3. **Passengers 2513_01 and 2514_01**: Cabins E/150/P and F/519/P\n",
    "4. **Passengers 3598_01 and 3599_01**: Cabins G/590/P and G/579/S\n",
    "5. **Passengers 6048_01 and 6048_01**: Cabins D/191/P and E/387/P\n",
    "6. **Passengers 7182_01 and 7183_01**: Cabins F/1489/P and G/1157/P\n",
    "7. **Passengers 7463_01 and 7469_01**: Cabins F/1544/P and G/1212/S\n",
    "    - Passenger 7463_01 also has the option of F/1424/S, but as 7463_01 and 7469_01 are the only passengers that can take F/1544/P and G/1212/S, logically 7463_01 must take one of F/1544/P or G/1212/S, and 7469_01 must take the other.\n",
    "8. **Passengers 7983_01 and 7995_01**: Cabins C/298/S and E/528/S\n",
    "\n",
    "#### Cases of passengers who have to share a Cabin with a member of their group, but there are multiple suitable Cabins\n",
    "1. **Passenger 1709_03**: Cabins F/326/S, D/61/S, E/127/S\n",
    "2. **Passenger 2092_03**: Cabins D/70/S, E/153/S, F/410/S\n",
    "3. **Passenger 3287_02**: Cabins G/522/S, F/621/S\n",
    "4. **Passenger 3411_02**: Cabins E/232/S, F/645/S\n",
    "5. **Passenger 8728_07**: Cabins F/1798/P, G/1416/P\n",
    "\n",
    "#### Other cases\n",
    "1. **Passengers 0348_02, 0364_02, and 0374_02**: Each of these passengers is in a group of 2 and can only take cabins E/20/P and E/21/P, meaning that one of them shares with their group member, and the other two take those cabins.\n",
    "2. With the knowledge of whether passenger 7353_03 shares with a group member or 7442_02\n",
    "    - Passenger 7353_03 takes C/270/S and then:\n",
    "    - Passenger 7368_01 takes D/235/P\n",
    "    - Passenger 7429_01 takes F/1424/S\n",
    "    - Passenger 7503_02 takes G/1206/S\n",
    "    - Passenger 7442_02 shares E/495/S with a group member\n",
    "\n",
    "    Or:\n",
    "    \n",
    "    - Passenger 7353_03 shares C/269/S with a group member, and then:\n",
    "    - Passenger 7368_01 takes C/270/S\n",
    "    - Passenger 7429_01 takes D/235/P\n",
    "    - Passenger 7503_02 takes F/1424/S\n",
    "    - Passenger 7442_02 takes G/1206/S\n",
    "\n",
    "## Further Assumptions\n",
    "\n",
    "There aren't more cabins available than we have assumed. While working on this project, I initially missed imputing many cabins due to the belief that cabin numbers could extend beyond what we observed. However, after assuming that cabins do not extend beyond the observed numbers, the passengers all fit within the given constraints. This seemed unlikely if there were more cabins available than assumed, as we did not find any passengers without options or rooms without matching passengers.\n",
    "\n",
    "In conclusion, this project has significantly improved the completeness of our dataset by reducing the number of missing Cabin values from 299 to just 29. These remaining cases present complex scenarios that require further inference or additional data to resolve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Appendix\n",
    "For the Evidence in the Appendix I will reuse the combined original dataframes without any imputations as to not misrepresent the underlying distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "df = pd.concat([training_data,test_data]) \n",
    "\n",
    "df = column_splits(df)\n",
    "df = df.sort_values(by = ['Group','GroupNumber'])\n",
    "df = df.reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1\n",
    "Evidence of passengers sharing a 'Group' implying that their cabin is on the same 'CabinSide'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with consistent cabin sides: 5825\n",
      "Number of rows with inconsistent cabin sides: 0\n"
     ]
    }
   ],
   "source": [
    "# Group by 'Group' and check if all non-NaN 'CabinSide' values within each group are the same\n",
    "consistent_count = 0\n",
    "inconsistent_count = 0\n",
    "\n",
    "# Iterate through each group\n",
    "for group, group_df in df.groupby('Group'):\n",
    "    if len(group_df) > 1:\n",
    "        # Get unique non-NaN CabinSide values\n",
    "        unique_sides = group_df['CabinSide'].dropna().unique()\n",
    "        \n",
    "        if len(unique_sides) <= 1:\n",
    "            # All rows in this group are consistent\n",
    "            consistent_count += len(group_df)\n",
    "        else:\n",
    "            # Some rows in this group are inconsistent\n",
    "            inconsistent_count += len(group_df)\n",
    "\n",
    "print(f\"Number of rows with consistent cabin sides: {consistent_count}\")\n",
    "print(f\"Number of rows with inconsistent cabin sides: {inconsistent_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2\n",
    "Evidence of passengers sharing a 'Group' implying that they have the same 'HomePlanet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with consistent home planets: 5825\n",
      "Number of rows with inconsistent home planets: 0\n"
     ]
    }
   ],
   "source": [
    "planet_consistent_count = 0\n",
    "planet_inconsistent_count = 0\n",
    "\n",
    "for group, group_df in df.groupby('Group'):\n",
    "    if len(group_df) > 1:\n",
    "    # Get unique non-NaN HomePlanet values\n",
    "        unique_home_planets = group_df['HomePlanet'].dropna().unique()\n",
    "        \n",
    "        if len(unique_home_planets) <= 1:\n",
    "            # All rows in this group are consistent in HomePlanet\n",
    "            planet_consistent_count += len(group_df)\n",
    "        else:\n",
    "            # Some rows in this group are inconsistent in HomePlanet\n",
    "            planet_inconsistent_count += len(group_df)\n",
    "\n",
    "print(f\"Number of rows with consistent home planets: {planet_consistent_count}\")\n",
    "print(f\"Number of rows with inconsistent home planets: {planet_inconsistent_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.3\n",
    "Evidence of passengers sharing a 'LastName' implying that they have the same 'HomePlanet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with consistent home planets by last name: 12468\n",
      "Number of rows with inconsistent home planets by last name: 0\n"
     ]
    }
   ],
   "source": [
    "planet_consistent_count = 0\n",
    "planet_inconsistent_count = 0\n",
    "\n",
    "for last_name, group_df in df.groupby('LastName'):\n",
    "    if len(group_df) > 1:  # Exclude last names with only one passenger\n",
    "        # Get unique non-NaN HomePlanet values\n",
    "        unique_home_planets = group_df['HomePlanet'].dropna().unique()\n",
    "        \n",
    "        if len(unique_home_planets) <= 1:\n",
    "            # All rows in this group are consistent in HomePlanet\n",
    "            planet_consistent_count += len(group_df)\n",
    "        else:\n",
    "            # Some rows in this group are inconsistent in HomePlanet\n",
    "            planet_inconsistent_count += len(group_df)\n",
    "\n",
    "print(f\"Number of rows with consistent home planets by last name: {planet_consistent_count}\")\n",
    "print(f\"Number of rows with inconsistent home planets by last name: {planet_inconsistent_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.4\n",
    "Evidence of children under the age of 13 having no 'Bills'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passengers under the age of 13:\n",
      "Total number of passengers under 13: 1030\n",
      "Number of passengers with bills = 0: 1030\n",
      "Number of passengers with bills != 0: 0\n"
     ]
    }
   ],
   "source": [
    "df['Bills'] = df['RoomService'] + df['FoodCourt'] + df['ShoppingMall'] + df['Spa'] + df['VRDeck']\n",
    "\n",
    "# Filter out rows with NaN values in Bills\n",
    "df_filtered = df[df['Bills'].notna()]\n",
    "\n",
    "# Check if passengers under the age of 13 have bills = 0\n",
    "under_13 = df_filtered[df_filtered['Age'] < 13]\n",
    "under_13_bills_zero = under_13['Bills'] == 0\n",
    "\n",
    "# Calculate summary statistics\n",
    "total_under_13 = len(under_13)\n",
    "bills_zero_under_13 = under_13_bills_zero.sum()\n",
    "bills_not_zero_under_13 = total_under_13 - bills_zero_under_13\n",
    "\n",
    "\n",
    "print(\"Passengers under the age of 13:\")\n",
    "print(f\"Total number of passengers under 13: {total_under_13}\")\n",
    "print(f\"Number of passengers with bills = 0: {bills_zero_under_13}\")\n",
    "print(f\"Number of passengers with bills != 0: {bills_not_zero_under_13}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A.5\n",
    "Evidence of those in 'CryoSleep' having no 'Bills'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passengers under the age of 13:\n",
      "Total Cryosleep: 4068\n",
      "Bills = 0: 4068\n",
      "Bills != 0: 0\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows with NaN values in Bills\n",
    "df_filtered = df[df['Bills'].notna()]\n",
    "\n",
    "# Check if passengers under the age of 13 have bills = 0\n",
    "cryo = df_filtered[df_filtered['CryoSleep'] == True]\n",
    "cryo_bills_zero = cryo['Bills'] == 0\n",
    "\n",
    "# Calculate summary statistics\n",
    "total_in_cryo = len(cryo)\n",
    "cryo_bills_zero = cryo_bills_zero.sum()\n",
    "cryo_bills_not_zero = total_in_cryo - cryo_bills_zero\n",
    "\n",
    "\n",
    "print(\"Passengers under the age of 13:\")\n",
    "print(f\"Total Cryosleep: {total_in_cryo}\")\n",
    "print(f\"Bills = 0: {cryo_bills_zero}\")\n",
    "print(f\"Bills != 0: {cryo_bills_not_zero}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A.6\n",
    "Evidence that 'Cabins' can only be shared by members of the same 'Group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total multi-member cabins: 1684\n",
      "Multi-member cabins shared by multiple groups: 0\n",
      "Multi-member cabins unique to one group: 1684\n"
     ]
    }
   ],
   "source": [
    "# Filter cabins with more than one member\n",
    "cabin_counts = df['Cabin'].value_counts()\n",
    "multi_member_cabins = cabin_counts[cabin_counts > 1].index\n",
    "\n",
    "# Group by Cabin and list unique groups for each Cabin with more than one member\n",
    "cabin_group_mapping = df[df['Cabin'].isin(multi_member_cabins)].groupby('Cabin')['Group'].unique()\n",
    "\n",
    "# Check if any Cabin is associated with more than one group\n",
    "shared_cabins = cabin_group_mapping[cabin_group_mapping.apply(lambda groups: len(groups) > 1)]\n",
    "\n",
    "# Summary statistics\n",
    "total_multi_member_cabins = len(cabin_group_mapping)\n",
    "shared_cabin_count = len(shared_cabins)\n",
    "unique_cabin_count = total_multi_member_cabins - shared_cabin_count\n",
    "\n",
    "print(f\"Total multi-member cabins: {total_multi_member_cabins}\")\n",
    "print(f\"Multi-member cabins shared by multiple groups: {shared_cabin_count}\")\n",
    "print(f\"Multi-member cabins unique to one group: {unique_cabin_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B\n",
    "Evidence of 'HomePlanets' Restricting Which 'CabinDeck' a passengers Cabin is on\n",
    "\n",
    "Grouping by HomePlanet and CabinDeck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HomePlanet  Earth  Europa  Mars\n",
      "CabinDeck                      \n",
      "A               0     346     0\n",
      "B               0    1124     0\n",
      "C               0    1081     0\n",
      "D               0     296   406\n",
      "E             583     197   508\n",
      "F            2426       0  1713\n",
      "G            3700       0     0\n",
      "T               0      10     0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Group by 'HomePlanet' and 'CabinDeck' and count occurrences\n",
    "deck_counts = df.groupby(['HomePlanet', 'CabinDeck']).size().reset_index(name='Count')\n",
    "\n",
    "# Pivot the table to get a better overview\n",
    "pivot_table = deck_counts.pivot(index='CabinDeck', columns='HomePlanet', values='Count').fillna(0).astype(int)\n",
    "\n",
    "print(pivot_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidence from passengers with no 'Bills' in 'Groups' with multiple 'CabinDecks'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CabinDecks for passengers with bills = 0 in groups with multiple CabinDecks:\n",
      "HomePlanet: Earth\n",
      "  CabinDeck: G, Count: 526\n",
      "HomePlanet: Mars\n",
      "  CabinDeck: F, Count: 160\n",
      "  CabinDeck: E, Count: 47\n",
      "HomePlanet: Europa\n",
      "  CabinDeck: B, Count: 11\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Group the dataframe by 'Group'\n",
    "for group, group_df in df.groupby('Group'):\n",
    "    # Check if there are multiple unique 'CabinDeck' values within the 'Group'\n",
    "    unique_decks = group_df['CabinDeck'].dropna().unique()\n",
    "    \n",
    "    # If there is more than one unique 'CabinDeck' within the 'Group'\n",
    "    if len(unique_decks) > 1:\n",
    "        # Find passengers with 'Bills' = 0 and a known 'HomePlanet' within the group\n",
    "        zero_bill_passengers = group_df[(group_df['Bills'] == 0) & (group_df['HomePlanet'].notna())]\n",
    "        \n",
    "        # Iterate through each passenger in zero_bill_passengers\n",
    "        for idx, passenger in zero_bill_passengers.iterrows():\n",
    "            # Get the HomePlanet and CabinDeck for the passenger\n",
    "            home_planet = passenger['HomePlanet']\n",
    "            cabin_deck = passenger['CabinDeck']\n",
    "            \n",
    "            # If the home planet is not already in the results dictionary, add it\n",
    "            if home_planet not in results:\n",
    "                results[home_planet] = []\n",
    "            \n",
    "            # Append the CabinDeck to the list of decks for the home planet\n",
    "            results[home_planet].append(cabin_deck)\n",
    "\n",
    "print(\"CabinDecks for passengers with bills = 0 in groups with multiple CabinDecks:\")\n",
    "for home_planet, cabin_decks in results.items():\n",
    "    # Count the occurrences of each cabin deck in the list\n",
    "    cabin_deck_counts = pd.Series(cabin_decks).value_counts().to_dict()\n",
    "    print(f\"HomePlanet: {home_planet}\")\n",
    "    # Print each cabin deck and its count for the home planet\n",
    "    for deck, count in cabin_deck_counts.items():\n",
    "        print(f\"  CabinDeck: {deck}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spaceenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
